{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8110437f-e0e9-4ec9-8d1d-be9c2e3b100e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n",
      "tensor([[[-1.2911, -1.8431,  0.7995, -0.1826],\n",
      "         [ 0.5918,  1.1331, -0.9081, -1.1502],\n",
      "         [ 0.5723, -0.7427,  1.4737,  1.0406]],\n",
      "\n",
      "        [[ 0.1881, -0.6787,  0.3107, -0.7220],\n",
      "         [-0.9186,  0.0493,  0.6530,  0.1796],\n",
      "         [ 0.8669, -1.3810, -0.7984,  1.3388]]])\n",
      "torch.Size([2, 3, 4])\n",
      "2\n",
      "24\n",
      "tensor([[-1.2911, -1.8431,  0.7995, -0.1826,  0.5918,  1.1331],\n",
      "        [-0.9081, -1.1502,  0.5723, -0.7427,  1.4737,  1.0406],\n",
      "        [ 0.1881, -0.6787,  0.3107, -0.7220, -0.9186,  0.0493],\n",
      "        [ 0.6530,  0.1796,  0.8669, -1.3810, -0.7984,  1.3388]])\n",
      "tensor([[-1.2911, -1.8431,  0.7995, -0.1826,  0.5918,  1.1331],\n",
      "        [-0.9081, -1.1502,  0.5723, -0.7427,  1.4737,  1.0406],\n",
      "        [ 0.1881, -0.6787,  0.3107, -0.7220, -0.9186,  0.0493],\n",
      "        [ 0.6530,  0.1796,  0.8669, -1.3810, -0.7984,  1.3388]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "## 入门\n",
    "\n",
    "import torch\n",
    "\n",
    "# 生成tensor\n",
    "x = torch.arange(12)    # 生成element从0~11的一维tensor\n",
    "x1 = torch.arange((12)) # 和上面写法等价\n",
    "\n",
    "print(x)\n",
    "print(x1)\n",
    "\n",
    "# 其他一些写法，zeros是全零，ones是全一，randn是标准高斯分布\n",
    "x2 = torch.zeros(2,3,4)  # 也可以写成 torch.zeros((2,3,4))\n",
    "print(x2)\n",
    "\n",
    "x3 = torch.ones((2,3,4))\n",
    "x4 = torch.randn(2,3,4)\n",
    "\n",
    "print(x3)\n",
    "print(x4)\n",
    "\n",
    "print(x4.shape)    # 返回torch.Size([2,3,4])\n",
    "print(x4.shape[0]) # 返回torch.Size([2,3,4])的第一维的值，也就是2\n",
    "print(x4.numel())  # 返回元素个数，也就是2*3*4=24\n",
    "\n",
    "x5 = x4.reshape(4,6)\n",
    "x6 = x4.reshape(4, -1)  # 用-1 代表自动计算...\n",
    "\n",
    "print(x5)\n",
    "print(x6)\n",
    "\n",
    "# 最后，tensor也可以手工指定\n",
    "x7 = torch.tensor([[1,2,3],[1,2,3]]) # 手工指定(2,3)的二维矩阵\n",
    "print(x7.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ef38474-01d3-436d-af36-a045db2b53af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([6, 4])\n",
      "torch.Size([3, 8])\n",
      "tensor([False,  True, False, False])\n",
      "tensor(True)\n",
      "tensor(66.)\n"
     ]
    }
   ],
   "source": [
    "## 运算符\n",
    "\n",
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))# 按位计算，只要形状相同就行\n",
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y  # **运算符是求幂运算, 这些运算是按元素，也就是element-wise\n",
    "\n",
    "torch.exp(x)  # 按位进行 exp运算\n",
    "\n",
    "# concatenate 连结操作\n",
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "\n",
    "# 在维度1进行连结，连结后维度1的值为X,Y维度1的值的和，其余维度不变(X,Y剩余维度应该相等)\n",
    "# 这里，第一维即dim=0，结果是3+3=6，第二维dim=1，不变，还是4\n",
    "a0 = torch.cat((X, Y), dim=0)  \n",
    "\n",
    "# 这里，第一维即dim=0，结果不变是3，第二维dim=1，4+4=8\n",
    "a1 = torch.cat((X, Y), dim=1)\n",
    "\n",
    "print(a0.shape)  # 应该是torch.Size([6, 4])\n",
    "print(a1.shape)  # 应该是torch.Size([3, 8])\n",
    "\n",
    "# 对 == 操作，也是按元素，只不过对应位置元素相同为True，否则为False\n",
    "a = (x==y)\n",
    "print(a)   # tensor([False,  True, False, False])\n",
    "print(a[1])\n",
    "\n",
    "# 对所有元素求和，得到一个单元素张量\n",
    "print(X.sum())  # tensor(66.), 即0~12的和\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d43995f8-9bf2-46a7-a47f-6a5a4f9b9d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1],\n",
      "        [1, 2],\n",
      "        [2, 3]])\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 0],\n",
      "        [0, 1],\n",
      "        [0, 2]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "## 广播机制\n",
    "# 核心就是如何在形状不相同的两个张量上执行按元素操作，机制如下\n",
    "# 1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；\n",
    "# 2. 对生成的数组执行按元素操作。\n",
    "\n",
    "# 在大多数情况下，我们将沿着数组中长度为1的轴进行广播\n",
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "\n",
    "# 为了让a和b能按元素操作，将(3,1)的a和(1,2)的b，都变成(3,2),此时a复制列，b复制行\n",
    "# a 从[[0],[1],[2]]变为[[0,0],[1,1],[2,2]]\n",
    "# b 从[0,1]变为[[0,1],[0,1],[0,1]]\n",
    "c = a + b\n",
    "print(c)\n",
    "print(c.shape)\n",
    "\n",
    "d = a * b\n",
    "print(d)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14eff2bb-cd3f-4496-938c-a350de993a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11)\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "tensor(7)\n",
      "tensor([[12, 12, 12, 12],\n",
      "        [12, 12, 12, 12]])\n"
     ]
    }
   ],
   "source": [
    "## 索引和切片\n",
    "\n",
    "x = torch.arange(12)\n",
    "print(x[-1])  # 通过下标访问\n",
    "\n",
    "y = x.reshape(-1, 4)\n",
    "print(y)\n",
    "print(y[1,3])  # 多维张量也可以通过下标访问\n",
    "y[1,1] = 9     # 通过下标赋值\n",
    "\n",
    "# \"0:2\"说明第一维要访问第1第2行， “:”说明要访问所有列(沿轴1的所有列)，12就是为所有元素赋值\n",
    "# liujia: 实际上，我感觉 : 代表了后面的所有维度的所有元素\n",
    "y[0:2, :] = 12    # 隐式用了广播机制 \n",
    "print(y[0:2, :])  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83f07598-4b1f-46b1-a219-9f9d61dea676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126693885409328\n",
      "126694087194592\n"
     ]
    }
   ],
   "source": [
    "## 节省内存\n",
    "\n",
    "X = torch.tensor([1, 2])\n",
    "Y = torch.tensor([2, 3])\n",
    "print(id(Y))\n",
    "Y = Y + X\n",
    "print(id(Y))   # Y的地址变了，总的来说，Y=Y+X 是先计算Y+X，然后为结果分配内存，最后使Y指向新内存\n",
    "\n",
    "# 要节省内存，就要使用\"原地操作\"\n",
    "# 我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如Y[:] = <expression>\n",
    "Z = torch.zeros_like(Y)  # Z的形状如同Y, 但值为0\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y             # : 和上面的注释一样，表明了所有维度的所有元素，用 X+Y的对应的element赋值\n",
    "print('id(Z):', id(Z))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4b2bfdb-23bb-42b7-a8f9-9671f99d9760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n",
      "tensor([2.5000])\n",
      "2.5\n",
      "2.5\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "## 转换为其他python对象，主要是转numpy\n",
    "\n",
    "A = X.numpy()        # 转为numpy的ndarray对象\n",
    "print(type(A))\n",
    "\n",
    "B = torch.tensor(A)  # 类似强制类型转换那样，将ndarray转为tensor\n",
    "print(type(B))\n",
    "\n",
    "a = torch.tensor([2.5])\n",
    "print(a)\n",
    "print(a.item())   # item()将大小为1的张量转为python的标量\n",
    "print(float(a))\n",
    "print(int(a))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6866aaa3-a240-40af-a8d8-1b278aab1b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
